---
layout: post
title: Designing a distributed caching service
categories: caching
---

Topic for today is to design a distributed caching service. Multiple variations are possible.
Here all of them will be explored.

## Clarification

1. what kind of cache - write back, write through, side cache
2. what consistency requirements
3. Avail SLO - each shard needs replication?
4. what kind of operations - get, put, remove, 
5. set TTL ?
6. simple kv or more Redis like?
7. support eviction to alleviate memory pressure? overflow to disk as option?
8. distributed - sharing out of scope or want us to manage sharding?
9. hot key / hot sub-shard - dynamic repartitioning required?
10. what is latency SLO?
11. If fronting a store, want to provide DDOS protection - throttling if too many requests to source? want to do load management to the source?
12. Is source shard 1:1 with cache shard ? (**)

## Cache pollution

Sequential access patterns can fill the cache will entries that will never be accessed again. To avoid that, we can require 2 hits for something
to be brought to cache. Or we use the LRU/K cache and first introduce in LRU/{N-1} level and as it gets more accesses promote it upwards 
eventually to LRU/0 level

## Cache eviction strategies

Cache eviction is necessary if the working set cannot fit in the memory allocated to the process.

**FIFO** sequential access pattern

**LRU** least recently used item is evicted. LRU can be implemented with a Queue and HashTable pointing key to queue node pointer. Better than LFU but has sequential access pattern problem.

**LFU** least frequently used item is evicted. Cannot be implemented with a Queue like LRU because 1 access to middle node does not immediately bring it to front of queue. We need to increment its access count and sort it accordingly. It is implemented using a SortedSet/BTree. If something is referenced heavily for a while and then forgotten it is not evicted for a long time.

**LRU/K and Segmented LRU** Evicting immediately from the primary stage becomes costly if its a mistake. Instead demote it from level0 to level1. If it gets evicted from level1 completely remove it from cache. This helps with avoiding sequential access pattern cache pollution as well. Better than plain LRU.

**LIRS**  Use a better distance metric than default used in LRU. Called IRR.

**CLOCK** Better than LRU because overhead is less. It is approximating, not equal to, LRU

> Why is CLOCK low overhead than LRU?

> Why is it approximation of LRU?

**CLOCK-pro** Like LIRS is to LRU, so is CLOCK-pro to CLOCK.

## How to implement TTL

Probably store the eviction timestamp with each entry. Priority queue based on eviction time.
When it is time to evict, remove the one with the smallest eviction timestamp. 
When entry is accessed, it is removed from priority queue, the entry's timestamp updated and re-inserted into the priority queue.
When doing a read from the cache if some entries have expired, it is removed from the cache.

## Linux's caching mechanisms

Linux's page cache is inspired from LRU/K and CLOCK-pro. It's not direct copy of either. Some details on [LWN](#2)

## References

1. [Wikipedia](https://en.wikipedia.org/wiki/Page_replacement_algorithm#Clock)
2. [LWN](https://lwn.net/Articles/147879/)