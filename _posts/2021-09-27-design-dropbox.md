---
layout: post
title: Design dropbox
---

# Functional requirements

1. Upload files
2. Download files
3. Support nesting of folders
4. Share files with other users
5. Sync files across devices
6. Files don't expire
   
# Non functional requirements

1. HA
2. Synchronized between devices within lag of 10 seconds
3. On conflict ask user to resolve the conflict
   

# Out of scope

1. Content de-duplication 
2. Files cannot be mutated in-place. Change leads to a full new version being uploaded

# Scale

max_files = 1,000,000
metadata_per_file = [file_id, parent_id, 4kb attributes]
metadata_size = 4 kilobytes
index_size = metadata_size * max_files
max_file_size = 100MB
users = 1,000,000
dau = 20% of users
uploads_per_user = 5 / day
uploads = uploads_per_user * dau
storage_growth_per_day = max_file_size * uploads * 1 day # adding 10 machines per day

# Focus areas

nothing particular

# High level design

1. There is metadata representing a filesystem structure for a user
2. For each file, the content is stored in a content-storage system
3. Max-file size is 100MB. With parallelization of synchronization we can keep sync time small even if many files are not synchronized
4. Optimized network transfer protocol can help in fast large syncs

## Synchronization algorithm

Synchronization is essentially a metadata sync problem. Once metadata is synced, files present in the update records can be fetched via the file downloader service.

1. Decoupling allows us to decouple the metadata architecture from the file storage architecture
2. Decoupling allows us to not immediately download a file. If user is clicking on some particular sub-folder, its file contents can be downloaded first.
3. Metadata updates are often smaller and so the end user perception of sync finishing and being usable very be high, giving beter interactive experience.

4. Synchronize metadata
5. if any node representing a file is part of the update record, fetch the file as well

```python
class Node:
    """
    Get a fs modification event from the agent that is monitoring the node A local
    file system

    Replicate the local event to the synchronization service.
    Each change record is represented via a unique lsn (node_id, record_sequence_number)
    """
    def on_local_event(modification_evt):
        sync_service_seqnum = self.sync_service.append_record(self.node_id, 
                                        self.record_seqnum, modification_evt)

        if sync_service_seqnum < self.record_seqnum:
            snapshot = self.create_snapshot(self.record_seqnum)
            self.sync_service.send_snapshot(snapshot)

        self.record_seqnum += 1

    def on_remote_event(remote_node_id, remote_record_seqnum, remote_event):
        remote_latest_record_seqnum = self.remote_change_log[remote_node_id]
        if remote_latest_record_seqnum == remote_record_seqnum - 1:
            self.remote_change_log[remote_node_id].append_record(remote_event)

    def on_init():
        (is_synced, service_lsn) = self.sync_service.send_current_lsn(, self.record_seqnum)
        self.sync_service.send_snapshot(self.create_snapshot())

class SyncService:
    """
    Receives change notifications from node
    Applies the notifications to the server side store
    """
    def on_node_event(node_id, record_seqnum, change_record):

```
## Change log capture vs. Direct database updates

Change log capture:
    1. change events are sent from node to sync_service
    2. sync_service's goal is to ensure that the log is replicated to other places and big sync'd with snapshots if need be.

Direct database updates:
    1. We don't want the node B to have to do a full diff of the local state and service state for propagating every update since that will be expensive. So, even in this approach we will make the DB state change, but the service will also write a change record in a different table or a Kafka queue.
       1. A Kafka queue is problematic because you have now introduced distributed transaction and need to worry about DB update happening but kafka enqueue failing. Since the sync service can die at any moment, it cannot wait around forever trying to write into the Kafka queue.
       2. That's why it is better for the change records to be persisted into the database itself. This will be a different table and you will need to rely on the database to support cross table transactions. We don't need cross partition transactions hence it is still possible to support this with a sharded database.
          1. (node_id, record_seqnum, operation)
       3. An even better mechanism would be for the database to generate a change log and then have the node B read from the change log generated by the database to sync its own state with it


## Unified approach

1. Node A sends change stream to SyncService
2. SyncService maintains a change stream to read from for the other nodes to sync up about node A

## Data Model

(user_id, node_id) => (node_attributes)

(node_id, file_id) => (file_parent, file_type, content_id)

(content_id) => (content_details)

